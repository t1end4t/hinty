# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

import typing
import typing_extensions
import baml_py

from . import stream_types, types, type_builder
from .parser import LlmResponseParser, LlmStreamParser
from .runtime import DoNotUseDirectlyCallManager, BamlCallOptions
from .globals import DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_RUNTIME as __runtime__


class BamlAsyncClient:
    __options: DoNotUseDirectlyCallManager
    __stream_client: "BamlStreamClient"
    __http_request: "BamlHttpRequestClient"
    __http_stream_request: "BamlHttpStreamRequestClient"
    __llm_response_parser: LlmResponseParser
    __llm_stream_parser: LlmStreamParser

    def __init__(self, options: DoNotUseDirectlyCallManager):
        self.__options = options
        self.__stream_client = BamlStreamClient(options)
        self.__http_request = BamlHttpRequestClient(options)
        self.__http_stream_request = BamlHttpStreamRequestClient(options)
        self.__llm_response_parser = LlmResponseParser(options)
        self.__llm_stream_parser = LlmStreamParser(options)

    def with_options(self,
        tb: typing.Optional[type_builder.TypeBuilder] = None,
        client_registry: typing.Optional[baml_py.baml_py.ClientRegistry] = None,
        collector: typing.Optional[typing.Union[baml_py.baml_py.Collector, typing.List[baml_py.baml_py.Collector]]] = None,
        env: typing.Optional[typing.Dict[str, typing.Optional[str]]] = None,
        tags: typing.Optional[typing.Dict[str, str]] = None,
        on_tick: typing.Optional[typing.Callable[[str, baml_py.baml_py.FunctionLog], None]] = None,
    ) -> "BamlAsyncClient":
        options: BamlCallOptions = {}
        if tb is not None:
            options["tb"] = tb
        if client_registry is not None:
            options["client_registry"] = client_registry
        if collector is not None:
            options["collector"] = collector
        if env is not None:
            options["env"] = env
        if tags is not None:
            options["tags"] = tags
        if on_tick is not None:
            options["on_tick"] = on_tick
        return BamlAsyncClient(self.__options.merge_options(options))

    @property
    def stream(self):
      return self.__stream_client

    @property
    def request(self):
      return self.__http_request

    @property
    def stream_request(self):
      return self.__http_stream_request

    @property
    def parse(self):
      return self.__llm_response_parser

    @property
    def parse_stream(self):
      return self.__llm_stream_parser
    
    async def ChatGPT(self, message: str,conversation_history: typing.Optional[typing.List["types.ConversationMessage"]] = None,file_content: typing.Optional[str] = None,tool_result: typing.Optional[str] = None,
        baml_options: BamlCallOptions = {},
    ) -> types.ChatResponse:
        # Check if on_tick is provided
        if 'on_tick' in baml_options:
            # Use streaming internally when on_tick is provided
            stream = self.stream.ChatGPT(message=message,conversation_history=conversation_history,file_content=file_content,tool_result=tool_result,
                baml_options=baml_options)
            return await stream.get_final_response()
        else:
            # Original non-streaming code
            result = await self.__options.merge_options(baml_options).call_function_async(function_name="ChatGPT", args={
                "message": message,"conversation_history": conversation_history,"file_content": file_content,"tool_result": tool_result,
            })
            return typing.cast(types.ChatResponse, result.cast_to(types, types, stream_types, False, __runtime__))
    async def Coder(self, user_message: str,files: typing.List["types.FileInfo"],codebase_context: typing.Optional["types.CodebaseContext"],conversation_history: typing.List["types.ConversationMessage"],
        baml_options: BamlCallOptions = {},
    ) -> types.CoderOutput:
        # Check if on_tick is provided
        if 'on_tick' in baml_options:
            # Use streaming internally when on_tick is provided
            stream = self.stream.Coder(user_message=user_message,files=files,codebase_context=codebase_context,conversation_history=conversation_history,
                baml_options=baml_options)
            return await stream.get_final_response()
        else:
            # Original non-streaming code
            result = await self.__options.merge_options(baml_options).call_function_async(function_name="Coder", args={
                "user_message": user_message,"files": files,"codebase_context": codebase_context,"conversation_history": conversation_history,
            })
            return typing.cast(types.CoderOutput, result.cast_to(types, types, stream_types, False, __runtime__))
    


class BamlStreamClient:
    __options: DoNotUseDirectlyCallManager

    def __init__(self, options: DoNotUseDirectlyCallManager):
        self.__options = options

    def ChatGPT(self, message: str,conversation_history: typing.Optional[typing.List["types.ConversationMessage"]] = None,file_content: typing.Optional[str] = None,tool_result: typing.Optional[str] = None,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.BamlStream[stream_types.ChatResponse, types.ChatResponse]:
        ctx, result = self.__options.merge_options(baml_options).create_async_stream(function_name="ChatGPT", args={
            "message": message,"conversation_history": conversation_history,"file_content": file_content,"tool_result": tool_result,
        })
        return baml_py.BamlStream[stream_types.ChatResponse, types.ChatResponse](
          result,
          lambda x: typing.cast(stream_types.ChatResponse, x.cast_to(types, types, stream_types, True, __runtime__)),
          lambda x: typing.cast(types.ChatResponse, x.cast_to(types, types, stream_types, False, __runtime__)),
          ctx,
        )
    def Coder(self, user_message: str,files: typing.List["types.FileInfo"],codebase_context: typing.Optional["types.CodebaseContext"],conversation_history: typing.List["types.ConversationMessage"],
        baml_options: BamlCallOptions = {},
    ) -> baml_py.BamlStream[stream_types.CoderOutput, types.CoderOutput]:
        ctx, result = self.__options.merge_options(baml_options).create_async_stream(function_name="Coder", args={
            "user_message": user_message,"files": files,"codebase_context": codebase_context,"conversation_history": conversation_history,
        })
        return baml_py.BamlStream[stream_types.CoderOutput, types.CoderOutput](
          result,
          lambda x: typing.cast(stream_types.CoderOutput, x.cast_to(types, types, stream_types, True, __runtime__)),
          lambda x: typing.cast(types.CoderOutput, x.cast_to(types, types, stream_types, False, __runtime__)),
          ctx,
        )
    

class BamlHttpRequestClient:
    __options: DoNotUseDirectlyCallManager

    def __init__(self, options: DoNotUseDirectlyCallManager):
        self.__options = options

    async def ChatGPT(self, message: str,conversation_history: typing.Optional[typing.List["types.ConversationMessage"]] = None,file_content: typing.Optional[str] = None,tool_result: typing.Optional[str] = None,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.baml_py.HTTPRequest:
        result = await self.__options.merge_options(baml_options).create_http_request_async(function_name="ChatGPT", args={
            "message": message,"conversation_history": conversation_history,"file_content": file_content,"tool_result": tool_result,
        }, mode="request")
        return result
    async def Coder(self, user_message: str,files: typing.List["types.FileInfo"],codebase_context: typing.Optional["types.CodebaseContext"],conversation_history: typing.List["types.ConversationMessage"],
        baml_options: BamlCallOptions = {},
    ) -> baml_py.baml_py.HTTPRequest:
        result = await self.__options.merge_options(baml_options).create_http_request_async(function_name="Coder", args={
            "user_message": user_message,"files": files,"codebase_context": codebase_context,"conversation_history": conversation_history,
        }, mode="request")
        return result
    

class BamlHttpStreamRequestClient:
    __options: DoNotUseDirectlyCallManager

    def __init__(self, options: DoNotUseDirectlyCallManager):
        self.__options = options

    async def ChatGPT(self, message: str,conversation_history: typing.Optional[typing.List["types.ConversationMessage"]] = None,file_content: typing.Optional[str] = None,tool_result: typing.Optional[str] = None,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.baml_py.HTTPRequest:
        result = await self.__options.merge_options(baml_options).create_http_request_async(function_name="ChatGPT", args={
            "message": message,"conversation_history": conversation_history,"file_content": file_content,"tool_result": tool_result,
        }, mode="stream")
        return result
    async def Coder(self, user_message: str,files: typing.List["types.FileInfo"],codebase_context: typing.Optional["types.CodebaseContext"],conversation_history: typing.List["types.ConversationMessage"],
        baml_options: BamlCallOptions = {},
    ) -> baml_py.baml_py.HTTPRequest:
        result = await self.__options.merge_options(baml_options).create_http_request_async(function_name="Coder", args={
            "user_message": user_message,"files": files,"codebase_context": codebase_context,"conversation_history": conversation_history,
        }, mode="stream")
        return result
    

b = BamlAsyncClient(DoNotUseDirectlyCallManager({}))